{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction\n",
    "\n",
    "In this section, we will introduce the concepts of MDP, Q-values, and V-values. These concepts are fundamental to the field of AI and machine learning, as they are used to model **decision-making problems** in various domains such as \"robotics\", \"finance\", and \"healthcare\".\n",
    "\n",
    "MDP stands for Markov Decision Process. It is a mathematical framework for modeling decision-making problems in which the outcomes are partly random and partly under the control of a decision-maker. MDPs are defined by a set of states, a set of actions, a reward function, and a transition function. The goal is to find a policy that maximizes the expected cumulative reward over time.\n",
    "\n",
    "Q-values and V-values are two important concepts in the context of MDPs. A Q-value represents the expected cumulative reward of taking a particular action in a particular state and following a specific policy thereafter. A V-value represents the expected cumulative reward of being in a particular state and following a specific policy thereafter. These values are used to evaluate and improve the policy of an agent in an MDP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: The Basics of MDPs\n",
    "\n",
    "In this section, we will explain the basic components of an MDP.\n",
    "\n",
    "An MDP is defined by \"a set of states\", \"a set of actions\", \"a reward function\", and \"a transition function\". The state space is the set of all possible states that the agent can be in. The action space is the set of all possible actions that the agent can take. The reward function defines the reward the agent receives for each action taken in a particular state. The transition function defines the probability of moving from one state to another state after taking a particular action.\n",
    "\n",
    "To illustrate these concepts, let's consider an example of a **robot that needs to navigate through a maze**. The robot can be in one of several states, such as at the start of the maze, at a junction in the maze, or at the end of the maze. This robot takes an action. With Probability of **0.8** It goes in that desired direction but with probability of **0.2** It goes in the perpendicular direction (0.1, 0.1 for each)!\n",
    "\n",
    "In an MDP, the agent interacts with the environment by selecting actions based on its current state and the expected future reward. The goal of the agent is to find a policy that maximizes the expected cumulative reward over time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**\n",
    "\n",
    "1. What are the state space, action space, reward function, and transition function of the robot in the maze example? Explain why you think each of these components is important for the robot to navigate through the maze.\n",
    "\n",
    "The state space is the coordinates of robot. The actions are going to a direction which is probably left right up down. Reward function can be defined by ourselves. For example the difference of manhattan distance between robot and exit. Transition function is the probability of going to some other state from a state by taking a specific action.\n",
    "\n",
    "With all of these components, we can simulate the environment and simulate or robot.\n",
    "\n",
    "2. Is our environment stochastic or deterministic? Why?! Stochastic; Because we don't know the outcome of our actions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define The MDP**:\n",
    "\n",
    "Based on your choice of rewards and transitions and the state space, define the MDP for the robot in the maze example. You can complete the following code to define the MDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Maze is: \n",
      " [[2 0 0 0 0]\n",
      " [0 1 1 0 1]\n",
      " [0 0 0 0 0]\n",
      " [0 1 1 1 3]]\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "\n",
    "# Definition of the maze\n",
    "maze = np.array([[2, 0, 0, 0, 0],\n",
    "                 [0, 1, 1, 0, 1],\n",
    "                 [0, 0, 0, 0, 0],\n",
    "                 [0, 1, 1, 1, 3]])\n",
    "\n",
    "print(\"Our Maze is: \\n\", maze)\n",
    "\n",
    "# Define the states and actions\n",
    "class Action(Enum):\n",
    "    LEFT = (0,-1)\n",
    "    RIGHT = (0,1)\n",
    "    UP = (-1,0)\n",
    "    DOWN = (1,0)\n",
    "    @staticmethod\n",
    "    def next_state(state: tuple[int, int], action) -> tuple[int, int]:\n",
    "        return (state[0] + action.value[0], state[1] + action.value[1])\n",
    "        \n",
    "    def perpendicular(self):\n",
    "        if self == Action.LEFT or self == Action.RIGHT:\n",
    "            return [Action.UP, Action.DOWN]\n",
    "        return [Action.LEFT, Action.RIGHT]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        if self == Action.LEFT:\n",
    "            return \"←\"\n",
    "        if self == Action.RIGHT:\n",
    "            return \"→\"\n",
    "        if self == Action.UP:\n",
    "            return \"↑\"\n",
    "        return \"↓\"\n",
    "        \n",
    "\n",
    "# Define the reward function\n",
    "def state_reward(state: tuple[int, int]) -> int:\n",
    "    #return (maze.shape[0] + maze.shape[1]) - (abs(maze.shape[0] - 1 - state[0]) + abs(maze.shape[1] - 1 - state[1]))\n",
    "    if state[0] == maze.shape[0] - 1 and state[1] == maze.shape[1] - 1:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# Define the transition probabilities\n",
    "def valid_next_state(state: tuple[int, int], action: Action) -> bool:\n",
    "    next_state = Action.next_state(state, action)\n",
    "    if next_state[0] < 0 or next_state[1] < 0:\n",
    "        return False\n",
    "    if next_state[0] >= maze.shape[0] or next_state[1] >= maze.shape[1]:\n",
    "        return False\n",
    "    if maze[next_state[0], next_state[1]] == 1:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def transition_probs(state1: tuple[int, int], action: Action, state2: tuple[int, int]) -> float:\n",
    "    next_predicted_state = Action.next_state(state1, action)\n",
    "    if not valid_next_state(state1, action) and next_predicted_state == state2:\n",
    "        return 0\n",
    "    # Check other states which we can go in\n",
    "    valid_other_states: list[tuple[int, int]] = []\n",
    "    for a in action.perpendicular():\n",
    "        if valid_next_state(state1, a):\n",
    "            valid_other_states.append(Action.next_state(state1, a))\n",
    "    # Normal move\n",
    "    if next_predicted_state == state2:\n",
    "        return 0.8\n",
    "    # Perpendicular move\n",
    "    if state2 in valid_other_states:\n",
    "        return 0.1\n",
    "    # Something else\n",
    "    return 0\n",
    "\n",
    "# Set the discount factor (for further use in v-value iteration and q-value iteration)\n",
    "discount = 0.9\n",
    "\n",
    "# Define the initial value function (you can simply set all to 0)\n",
    "values = np.zeros(maze.shape)\n",
    "values[-1, -1] = 1\n",
    "\n",
    "# Define the initial Q function (you can simply set all to 0)\n",
    "q_values = np.zeros(maze.shape + (len(Action),))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Computing V-values\n",
    "\n",
    "In this section, we will explain how to compute V-values for an MDP using the Bellman equation.\n",
    "\n",
    "The Bellman equation is a recursive equation that expresses the value of a state in terms of the values of its successor states. It is defined as:\n",
    "\n",
    "$$V(s) = R(s) + \\gamma * \\max_a (\\sum_{s'} P(s, a, s') * V(s'))$$\n",
    "\n",
    "where V(s) is the value of state s, R(s) is the reward for being in state s, γ is the discount factor that determines the importance of future rewards, P(s, a, s') is the probability of moving from state s to state s' after taking action a, and max_a is the maximum over all possible actions a.\n",
    "\n",
    "To compute the V-values for an MDP, we start with an initial estimate of the V-values and update them iteratively using the Bellman equation until they converge to the true values. The update rule is:\n",
    "\n",
    "$$V(s) \\leftarrow R(s) + \\gamma * \\max_a (\\sum_{s'} P(s, a, s') * V(s'))$$\n",
    "\n",
    "We can use dynamic programming algorithms such as value iteration or policy iteration to compute the V-values.\n",
    "\n",
    "We can use the Bellman equation to compute the V-values for each state in the maze. The V-values represent the expected cumulative reward that the robot can obtain if it starts from that state and follows an optimal policy thereafter. Complete the code below:\n",
    "\n",
    "(**Note:** your final result can be slightly different from the result printed below and it's okay!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33622712 0.41509521 0.57652112 0.80072378 0.57652112]\n",
      " [0.41509521 0.         0.         0.96798608 0.        ]\n",
      " [0.57652112 0.69694998 0.96798608 1.34442512 1.74625885]\n",
      " [0.41509521 0.         0.         0.         2.25730637]]\n"
     ]
    }
   ],
   "source": [
    "# Initiate again\n",
    "values = np.zeros(maze.shape)\n",
    "values[-1, -1] = 1\n",
    "# Do the thing\n",
    "ITERATIONS = 100\n",
    "for _ in range(ITERATIONS):\n",
    "    for (x, y), _ in np.ndenumerate(values):\n",
    "        if maze[x, y] == 1: # void\n",
    "            continue\n",
    "        best_action = 0 # The arg max\n",
    "        for a in Action:\n",
    "            valid_next_states: list[tuple[int, int]] = [] # States which are valid from this state\n",
    "            for a2 in Action:\n",
    "                if valid_next_state((x, y), a2):\n",
    "                    next_state = Action.next_state((x, y), a2)\n",
    "                    valid_next_states.append(next_state)\n",
    "            s = 0 # sum\n",
    "            for next_state in valid_next_states:\n",
    "                s += transition_probs((x, y), a, next_state) * values[next_state[0], next_state[1]]\n",
    "            best_action = max(best_action, s)\n",
    "        values[x, y] = state_reward((x, y)) + discount * best_action\n",
    "\n",
    "# Print the V-values\n",
    "print(values)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Computing Q-values\n",
    "\n",
    "In this section, we will explain how to compute Q-values for an MDP using the Bellman equation.\n",
    "\n",
    "The Q-values represent the expected cumulative reward that the robot can obtain if it starts from a particular state and takes a particular action, and then follows an optimal policy thereafter. The Q-values can be computed using the Bellman equation as follows:\n",
    "\n",
    "$$Q(s, a) = \\sum_{s'} P(s, a, s') (R(s, a) + \\gamma \\max_{a'} (Q(s', a')))$$\n",
    "\n",
    "where Q(s, a) is the Q-value of state-action pair (s, a), R(s, a) is the reward for taking action a in state s, γ is the discount factor that determines the importance of future rewards, P(s, a, s') is the probability of moving from state s to state s' after taking action a, max_a' is the maximum over all possible actions a' in state s', and sum_s' is the sum over all possible successor states s' of state s.\n",
    "\n",
    "To compute the Q-values for an MDP, we start with an initial estimate of the Q-values and update them iteratively using the Bellman equation until they converge to the true values. The update rule is:\n",
    "\n",
    "$$Q(s, a) = \\sum_{s'} P(s, a, s') (R(s, a) + \\gamma \\max_{a'} (Q(s', a')))$$\n",
    "\n",
    "We can use dynamic programming algorithms such as Q-learning or SARSA to compute the Q-values.\n",
    "\n",
    "\n",
    "We can use the Q-learning algorithm to compute the Q-values for each state-action pair in the maze. The Q-values represent the expected cumulative reward that the robot can obtain if it starts from a particular state and takes a particular action, and then follows an optimal policy thereafter. Complete the code below:\n",
    "\n",
    "(**Note:** your final result can be slightly different from the result printed below and it's okay!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.02988686 0.2689817  0.02988686 0.2689817 ]\n",
      "  [0.19366682 0.33207617 0.06571787 0.06571787]\n",
      "  [0.23909484 0.4612169  0.08753897 0.08753897]\n",
      "  [0.40177117 0.40177117 0.08301904 0.64057903]\n",
      "  [0.4612169  0.         0.05765211 0.05765211]]\n",
      "\n",
      " [[0.06571787 0.06571787 0.19366682 0.33207617]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.15445072 0.15445072 0.4612169  0.77438887]\n",
      "  [0.         0.         0.         0.        ]]\n",
      "\n",
      " [[0.05977371 0.4612169  0.28927524 0.28927524]\n",
      "  [0.33207617 0.55755998 0.11120452 0.11120452]\n",
      "  [0.40144319 0.77438887 0.14697901 0.14697901]\n",
      "  [0.62725498 1.07554009 0.75298562 0.19542564]\n",
      "  [0.93691493 0.16252606 0.09679861 1.39700708]]\n",
      "\n",
      " [[0.04150952 0.04150952 0.33207617 0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.22573064 0.22573064 1.8058451  0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "# Re-initiate\n",
    "q_values = np.zeros(maze.shape + (len(Action),))\n",
    "# Compute Q-Values using Bellman equations\n",
    "ITERATIONS = 100\n",
    "for _ in range(ITERATIONS):\n",
    "    for (x, y, a), _ in np.ndenumerate(q_values):\n",
    "        if maze[x, y] == 1: # void\n",
    "            continue\n",
    "        s = 0 # Sum\n",
    "        valid_next_states: list[tuple[int, int]] = []\n",
    "        for action in Action:\n",
    "            if valid_next_state((x, y), action):\n",
    "                next_state = Action.next_state((x, y), action)\n",
    "                valid_next_states.append(next_state)\n",
    "        for next_state in valid_next_states:\n",
    "            action = list(Action)[a]\n",
    "            best_q = 0\n",
    "            for a2 in Action:\n",
    "                best_q = max(best_q, q_values[next_state[0], next_state[1], list(Action).index(a2)])\n",
    "            s += transition_probs((x, y), action, next_state) * (state_reward((x, y)) + discount * best_q)\n",
    "        q_values[x, y, a] =  s\n",
    "\n",
    "# Print the Q-values\n",
    "print(q_values)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Visualizing the Optimal Policy\n",
    "\n",
    "Now that we have computed the Q-values, we can use them to find the optimal policy, which is the sequence of actions that the robot should take in each state to maximize its expected reward. We can visualize the optimal policy as arrows in a grid, where each arrow corresponds to the action with the highest Q-value in the corresponding state. Complete the code below:\n",
    "\n",
    "(**Note:** your final result can be slightly different from the result printed below and it's okay!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['→', '→', '→', '↓', '←'], ['↓', '×', '×', '↓', '×'], ['→', '→', '→', '→', '↓'], ['↑', '×', '×', '×', '↑']]\n"
     ]
    }
   ],
   "source": [
    "# Compute the optimal policy\n",
    "\n",
    "policy: list[list[str]] = []\n",
    "for i in range(maze.shape[0]):\n",
    "    policy_row: list[str] = []\n",
    "    for j in range(maze.shape[1]):\n",
    "        if maze[i, j] == 1:\n",
    "            policy_row.append(\"×\")\n",
    "            continue\n",
    "        q_values_of_state = list(q_values[i, j])\n",
    "        action = list(Action)[q_values_of_state.index(max(q_values_of_state))]\n",
    "        policy_row.append(str(action))\n",
    "    policy.append(policy_row)\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ → → ↓ ←\n",
      "↓ × × ↓ ×\n",
      "→ → → → ↓\n",
      "↑ × × × ↑\n",
      "\n",
      "\n",
      "S P P P P \n",
      "P X X P X \n",
      "P P P P P \n",
      "P X X X G \n"
     ]
    }
   ],
   "source": [
    "for row in policy:\n",
    "    print(' '.join(row))\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "for row in maze:\n",
    "    for cell in row:\n",
    "        if cell == 0:\n",
    "            print(\"P\", end=\"\")\n",
    "        if cell == 1:\n",
    "            print(\"X\", end=\"\")\n",
    "        if cell == 2:\n",
    "            print(\"S\", end=\"\")\n",
    "        if cell == 3:\n",
    "            print(\"G\", end=\"\")\n",
    "        print(\" \", end=\"\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de5a9b8c93ff7a706bdcd632ca115386538201a0be2d2c452384957fc2bef200"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
