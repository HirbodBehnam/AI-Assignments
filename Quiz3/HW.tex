% !TEX program = xelatex
\documentclass[]{article}
\usepackage{commons/course}

\begin{document}
\printheader

\smalltitle{سوال 1}
\\\noindent
معادلات بلمن یک سری معادلات بازگشتی هستند که به ما کمک می‌کنند مرحله به مرحله به یک متغیری همگرا شویم.
این معادلات در هوش و شبکه کاربرد دارند. به عنوان مثال در هوش و
\lr{MDP}ها
استفاده می‌شوند برای بدست آوردن
\lr{V} و \lr{Q values}
های مختلف استفاده می‌شوند. این الگوریتم به کمک تقریب‌هایی که برای حالات دیگر بدست اورده‌ایم به ما کمک می‌کند
که تقریب‌هایمان برای حالت فعلی را بهتر کنیم. طوری که در
\lr{iteration}
بی‌نهایت متغیری که دنبال آن بودیم به مقدار واقعی آن همگرا می‌شود.


\smalltitle{سوال 2}
\\\noindent
فرق اساسی که
\lr{Q learning}
با
\lr{SARSA}
دارد این است که در
\lr{Q learning}
همیشه بیشترین
\lr{Q value}
را بر می‌گرداند ولی در
\lr{SARSA}
مثلا به کمک
$\epsilon greedy$
حالت بعدی بررسی می‌شود.
فرمول
\lr{Q learning}
به صورت زیر است:
\begin{gather*}
    Q(s_t, a_t) = Q(s_t, a_t) + \alpha (r + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))
\end{gather*}
و
\lr{SARSA}
به صورت زیر است:
\begin{gather*}
    Q(s_t, a_t) = Q(s_t, a_t) + \alpha (r + \gamma \max_a Q(s_{t+1}, a_{t}) - Q(s_t, a_t))
\end{gather*}
\smalltitle{سوال 3}
\\\noindent
در ابتدای کار ما هیچ اطلاعاتی از محیط نداریم. این موضوع باعث می‌شود که به صورت رندوم
\lr{action}های
مختلف را انتخاب کنیم و
\lr{explore}
کنیم. در ابتدا این موضوع مشکلی ندارد و خیلی خوب است چرا که راه‌های مختلف را اکتشاف می‌کنیم.
اما در آینده می‌تواند مشکل ساز باشد. چرا که در زمان خیلی دور ما اطلاعات خوبی از محیط داریم
که مثلا در صورتی که فلان کار را انجام دهیم،
\lr{reward}
بیشتری می‌گیریم. منطقی است که در زمانی که اطلاعات خوبی از محیط داریم از مسیرهایی برویم که
\lr{reward}
بیشتری می‌گیریم. ولی در اول بهتر است که رندوم عمل کنیم.

برای اینکه این مشکل را حل کنیم از الگوریتم‌های
$\epsilon greedy$
استفاده می‌کنیم. بدین منظور که با احتمال
$\epsilon$
یک حرکت رندوم به جای بهترین حرکت انجام می‌دهیم. در ابتدا اپسیلون را مقدار زیادی قرار می‌دهیم و در ادامه کم می‌کنیم آنرا.
\end{document}
